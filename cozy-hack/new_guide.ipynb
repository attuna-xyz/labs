{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cbfc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries \n",
    "from prompt_learner.tasks.classification import ClassificationTask\n",
    "from prompt_learner.examples.example import Example\n",
    "from prompt_learner.prompts.prompt import Prompt\n",
    "from prompt_learner.prompts.cot import CoT\n",
    "from prompt_learner.templates.markdown import MarkdownTemplate\n",
    "from prompt_learner.selectors.random_sampler import RandomSampler\n",
    "from prompt_learner.evals.metrics.accuracy import Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1176c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Define the classification task for tagging GitHub issues\n",
    "task_description = \"Classify incoming GitHub issues into various categories.\"\n",
    "allowed_labels = [\"bug\", \"enhancement\", \"question\", \"documentation\", \"duplicate\", \"invalid\", \"wontfix\"]\n",
    "classification_task = ClassificationTask(description=task_description, allowed_labels=allowed_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6dd44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Add example issues to the classification task\n",
    "classification_task.add_example(Example(text=\"The app crashes on startup.\", label=\"bug\"))\n",
    "classification_task.add_example(Example(text=\"Please add a dark mode feature.\", label=\"enhancement\"))\n",
    "classification_task.add_example(Example(text=\"How do I install the package?\", label=\"question\"))\n",
    "classification_task.add_example(Example(text=\"This documentation is outdated.\", label=\"documentation\"))\n",
    "classification_task.add_example(Example(text=\"Issue #123 is a duplicate of this issue.\", label=\"duplicate\"))\n",
    "classification_task.add_example(Example(text=\"This report does not describe a real issue.\", label=\"invalid\"))\n",
    "classification_task.add_example(Example(text=\"I don't want you to work on this anymore.\", label=\"wontfix\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c69bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Create a Markdown template for the prompt\n",
    "template = MarkdownTemplate(task=classification_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9a61a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Select examples using a random sampler\n",
    "sampler = RandomSampler(num_samples=3, task=classification_task)\n",
    "sampler.select_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Assemble the prompt using CoT\n",
    "gpt_prompt = CoT(template=template)\n",
    "gpt_prompt.assemble_prompt()\n",
    "print(gpt_prompt.prompt)  # Display the assembled prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Compute accuracy on the classification task\n",
    "acc, num_total_samplers = Accuracy(classification_task).compute(gpt_prompt, OpenAI())\n",
    "print(\"Validation accuracy: \", acc, \" with \", num_total_samplers, \" eval samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffc8d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Add more example issues for testing\n",
    "classification_task.add_example(Example(text=\"Add a feature to support user-defined shortcuts.\", label=\"enhancement\"))\n",
    "classification_task.add_example(Example(text=\"I don't understand how to use this function.\", label=\"question\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a20b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Reassemble the prompt with the new examples\n",
    "sampler = RandomSampler(num_samples=3, task=classification_task)\n",
    "sampler.select_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403e23e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_prompt = CoT(template=template)\n",
    "gpt_prompt.assemble_prompt()\n",
    "print(gpt_prompt.prompt)  # Display the reassembled prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6b62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Compute test accuracy\n",
    "acc, num_total_samplers = Accuracy(classification_task).compute(gpt_prompt, OpenAI(), test=True)\n",
    "print(\"Test accuracy: \", acc, \" with \", num_total_samplers, \" eval samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2140228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Add inference for a new issue to classify\n",
    "gpt_prompt.add_inference(\"The sorting function does not work as expected.\")\n",
    "answer = classification_task.predict(OpenAI(), gpt_prompt.prompt)\n",
    "print(\"Predicted label for new issue:\", answer)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
